{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spatial Analysis with [PySAL](https://pysal.org)\n",
    "\n",
    "Examples used here are based on the [excellent tutorials](http://darribas.org/materials.html) by Dani Arribas-Bel.\n",
    "\n",
    "### Let's start with the PySal *viz* component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import libpysal \n",
    "from libpysal import weights\n",
    "from pysal.explore import esda\n",
    "from pysal.viz import mapclassify\n",
    "#from pysal.viz.splot.mapping import vba_choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10] # change standard figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a [geopackage](http://www.geopackage.org) with districts in [Belo Horizonte](https://en.wikipedia.org/wiki/Belo_Horizonte), reproject it to [EPSG:3857](https://epsg.io/3857), and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = gpd.read_file('https://github.com/darribas/gds_ufmg19/raw/master/data/bh.gpkg').to_crs(epsg=3857)\n",
    "db.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn [color palettes](https://seaborn.pydata.org/tutorial/color_palettes.html)\n",
    "\n",
    "Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('viridis', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('coolwarm', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('Set2', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning colors to values: Classification \n",
    "\n",
    "In the raster lecture, we already used a non-linear mapping from data values to colors. There are more elaborate ways to do this, though, particularly if we want to classify the data so that we can reduce the number of color values on the map to preserve readability. We'll go through a few ways to do this (giving you some cartography skills about [Choropleth Maps](https://onlinelibrary-wiley-com.zorac.aub.aau.dk/doi/abs/10.1002/9781118786352.wbieg0951) along the way), starting with\n",
    "\n",
    "**Equal intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.EqualInterval(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make ourselves a little function that will nicely show where the boundaries between the classes are when we want to compare the different classification methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClassification(classi):\n",
    "    # Set up the figure\n",
    "    f, ax = plt.subplots(1, figsize=(9, 6))\n",
    "    # Plot the kernel density estimation (KDE)\n",
    "    sns.kdeplot(db['Average Monthly Wage'], shade=True)\n",
    "    # Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "    sns.rugplot(db['Average Monthly Wage'], alpha=0.5)\n",
    "    # Loop over each break point and plot a vertical red line\n",
    "    for cut in classi.bins:\n",
    "        plt.axvline(cut, color='red', linewidth=0.75)\n",
    "    # Title\n",
    "    ax.set_title(classi.name)\n",
    "    # Display image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantiles**\n",
    "\n",
    "The equal intervals method splits up the data into, *ahem*, equal intervals. Quantiles instead arranges the class boundaries so that we have the same number of data points in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.Quantiles(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Fisher-Jenks](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization)** is a data clustering method designed to determine the \"best\" arrangement of values, such that natural breaks are identified and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.FisherJenks(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplot** should sound familiar (if not, go back to the exploratory data analysis notebook). This method will use the same breaks use for the boxplot to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.BoxPlot(db['Average Monthly Wage'])\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the classification relate to the box plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, axs = plt.subplots(2, figsize=(10, 10), gridspec_kw = {'height_ratios':[3, 1]})\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db['Average Monthly Wage'], shade=True, ax=axs[0])\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db['Average Monthly Wage'], alpha=0.5, ax=axs[0])\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    axs[0].axvline(cut, color='red', linewidth=0.75)\n",
    "# Box-Plot\n",
    "axs[1].boxplot(db['Average Monthly Wage'], vert=False)\n",
    "# Set X axis manually\n",
    "axs[1].set_xlim(axs[0].get_xlim())\n",
    "# Title\n",
    "axs[0].set_title(classi.name)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some\n",
    "\n",
    "## Choropleth maps\n",
    "\n",
    "Use the classification methods to assign colors to features on the map.\n",
    "\n",
    "üèã Which of the following maps is the best? Which one is correct? Let's discuss during the Q&A!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for classification in ['equal_interval', 'quantiles', 'fisher_jenks']:\n",
    "    f, ax = plt.subplots(1, figsize=(14, 14))\n",
    "    db.plot(column='Average Monthly Wage', scheme=classification, ax=ax, legend=True)\n",
    "    ctx.add_basemap(ax, url=ctx.providers.Stamen.Toner)\n",
    "    ax.set_axis_off()\n",
    "    plt.axis('equal')\n",
    "    plt.title(classification)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Spatial Weights](http://darribas.org/gds_scipy16/ipynb_md/03_spatial_weights.html)\n",
    "\n",
    "Spatial weights are central components of many areas of spatial analysis. In general terms, for a spatial data set composed of *n* locations (points, areal units, network edges, etc.), the spatial weights matrix expresses the potential for interaction between observations at each pair *i,j* of locations. There is a rich variety of ways to specify the structure of these weights, and PySAL supports the creation, manipulation and analysis of spatial weights matrices across three different general types:\n",
    "\n",
    "- Contiguity Based Weights\n",
    "- Distance Based Weights\n",
    "- Kernel Weights\n",
    "\n",
    "Let's take a look at a lattice example with fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a regular 3x3 lattice and draw it here\n",
    "w = weights.lat2W(3, 3, rook=True)\n",
    "# Get points in a grid\n",
    "l = np.arange(3)\n",
    "xs, ys = np.meshgrid(l, l)\n",
    "# Set up store\n",
    "polys = []\n",
    "# Generate polygons\n",
    "for x, y in zip(xs.flatten(), ys.flatten()):\n",
    "    poly = Polygon([(x, y), (x+1, y), (x+1, y+1), (x, y+1)])\n",
    "    polys.append(poly)\n",
    "# Convert to GeoSeries\n",
    "polys = gpd.GeoSeries(polys)\n",
    "gdf = gpd.GeoDataFrame({'geometry': polys, \n",
    "                        'id': ['P-%s'%str(i).zfill(2) for i in range(len(polys))]})\n",
    "w.remap_ids(gdf['id'].values)\n",
    "# Annotate & Visualise\n",
    "ax = polys.plot(edgecolor='k', facecolor='w')\n",
    "[plt.text(x, y, t, \n",
    "          verticalalignment='center',\n",
    "          horizontalalignment='center') for x, y, t in zip(\n",
    "         [p.centroid.x for p in polys],\n",
    "         [p.centroid.y for p in polys],\n",
    "         [i for i in gdf['id']])]\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a contiguity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(w.full()[0], \n",
    "             index=gdf['id'],\n",
    "             columns=gdf['id'],\n",
    "            ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world data: Belo Horizonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen = weights.Queen.from_dataframe(db)\n",
    "pd.DataFrame(w_queen.full()[0], \n",
    "             index=db['CD_GEOCMU'],\n",
    "             columns=db['CD_GEOCMU'],\n",
    "            ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Rook's case vs. Queen's case: \n",
    "\n",
    "- In Rook's case, only sees shared **edges** lead to a connection\n",
    "- In Queen's case, also shared **vertices** (points) lead to a connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Autocorrelation\n",
    "\n",
    "For more methods to analyse spatial autocorrelation, check out https://geographicdata.science/book/notebooks/06_spatial_autocorrelation.html\n",
    "\n",
    "We'll just do a quick analysis using **[Moran's I](https://en.wikipedia.org/wiki/Moran%27s_I)**\n",
    "\n",
    "We want to analyse the spatial autocorrelation of the industry diversity in Belo Horizonte. For comparison, let's create fake data with random industry diversity data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "db['Random Industry Diversity'] = db['Industry Diversity'].sample(frac=1).values\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = esda.Moran(db['Industry Diversity'], w_queen)\n",
    "moran.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_shuffled = esda.Moran(db['Random Industry Diversity'], w_queen)\n",
    "moran_shuffled.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_shuffled.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí™ Exercise\n",
    "\n",
    "Write some code to:\n",
    "\n",
    "1. Download the countries dataset from https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\n",
    "2. Make a choropleth map of the population density (column `POP_EST` divided by country area)\n",
    "3. Calculate Moran's I to understand whether there is spatial autocorrelation of the gross domestic product per capita (`GDP_MD` / `POP_EST`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
